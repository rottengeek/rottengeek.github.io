---
layout: post
title: "scrapy和requests源码解析"
subtitle: '编码与解码'
author: "Daituodi"
header-img: "img/post-bg-dreamer.jpg"
header-mask: 0.4
tags:
  - Scrapy
---


在做python开发和网络爬虫，相信大家肯定遇到过字符串编码问题，本文就从字符串编码的基本概念和简单的编码解码讲起，再通过查看requests库和scrapy源码，分析它们是如何处理字符串编码的问题，首先是字符串编码的基本概念，如下图

## 字符串编码基本概念
![字符编码](http://rottengeek.github.io/img/scrapy/字符编码.png)

![字符编码2](http://rottengeek.github.io/img/scrapy/字符编码2.png)


![字符编码2](http://rottengeek.github.io/img/scrapy/字符编码2.png)

## 字符串编码解码过程

![字符编码3](http://rottengeek.github.io/img/scrapy/字符编码3.png)





首先需要知道什么是Unicode，字符在内存中都是二进制的01，统一使用Unicode编码，当需要保存到硬盘或者传输的时候，就转换为指定编码(比如UTF8)！

Python以Unicode编码为转换中介，进行字符串的转码。（这里不赘述Python2个python3字符串编码的区别）

如果有需要可以看这篇文章 ：[Python2与Python3的字符编码与解码](https://www.jianshu.com/p/19c74e76ee0a)



接下来了解一下什么是编码，解码：

编码(encode)：在Unicode中，每一个字符都有一个唯一的数字表示，那么将Unicode字符串转换为特定字符编码（ASCII、UTF-8、GBK）对应的字节串的过程和规则就是编码。

解码(decode)：将特定字符编码（ASCII、UTF-8、GBK）的字节串转换为对应的Unicode字符串的过程和规则就是解码。

字符需要'有效'传输，所以需要将字符编码为二进制的字节序列，解码就是从二进制序列，按照指定的规则，恢复出原始的内容(Unicode)。

指定字节序列→解码decode→字符(Unicode编码)

字符(Unicode编码)→编码encode→指定字节序列

以python3 decode和encode为例子：

```python
              decode                    encode
str(byte类型) ---------> str(Unicode) ---------> str(byte类型)

>>> u = '中文'                 # 指定字符串类型对象u 

>>> str1 = u.encode('gb2312')  # 以gb2312编码对u进行编码，获得bytes类型对象
>>> print(str1)
b'\xd6\xd0\xce\xc4'

>>> str2 = u.encode('gbk')     # 以gbk编码对u进行编码，获得bytes类型对象
>>> print(str2)
b'\xd6\xd0\xce\xc4'
>>> str3 = u.encode('utf-8')   # 以utf-8编码对u进行编码，获得bytes类型对象
>>> print(str3)
b'\xe4\xb8\xad\xe6\x96\x87'

>>> u1 = str1.decode('gb2312') # 以gb2312编码对字符串str进行解码，获得字符串类型对象
>>> print('u1')
'中文'

>>> u2 = str1.decode('utf-8')  # 报错，因为str1是gb2312编码的
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xd6 in position 0: invalid continuation byte
```

## requests库的网页编码的处理

相信很多同学都用过python的requests库，当我们用requests库请求网页的时候，requests库给我们返回了一个response对象，如果我们要获取网页的源代码我们都使用**response.text**，有时候会遇到乱码，这时候又应该怎么处理呢，为什么会生成乱码呢？

首先我们从**response.encoding**这个属性讲起，那我们可以看看requests库是怎么生成这个encoding

在 **requests/adapters.py**中有一个构造response的函数，我们可以发现response.encoding是从**response.headers**中获取的

![requests源码1](http://rottengeek.github.io/img/scrapy/requests源码1.jpg)

接下来 我们可以找到**get_encoding_from_headers**这个函数

![requests源码2](http://rottengeek.github.io/img/scrapy/requests源码2.jpg)

可以发现其实编码类型就是从content-type里获取的，如果有的charset那么自然获得的是utf-8，如果没有就会返回ISO-8859-1（向下兼容ASCII），这就是很多时候**response.text**出现乱码的原因

![content-type1](http://rottengeek.github.io/img/scrapy/content-type1.jpg)

具体原因如下

```python
# response.content就是字节流 通过解码获得我们想要的response.text
response.text = response.content.decode(response.encoding)
```

那我们在代码中看到get_encoding_from_headers会返回None，那是不是返回None的时候就会报错呢，其实requests库做了一个处理，代码如下

```python
if self.encoding is None:
    encoding = self.apparent_encoding
```

这就又引入了apparent_encoding这个属性，那它又是什么呢，其实这就是requests库利用chardet对字节流编码进行了猜测，有兴趣的同学也可以看下chardet的源码，所以一般情况下response.apparent_encoding会比response.encoding来的更准确些，所以当我们遇到乱码的时候，使用response.encoding = response.apparent_encoding一般都能解决，但是有时候chardet对编码的猜测还是可能出现错误的，这样还是会出现乱码，所有需要具体问题具体分析。

```python
@property
def apparent_encoding(self):
  	"""The apparent encoding, provided by the chardet library."""
  	return chardet.detect(self.content)['encoding']
```

## Scrapy框架的网页编码的处理

接下来 我们来看下scrapy框架又是怎么处理网页编码的，在很多时候，用过scrapy的同学会知道，一次请求后，我们会获得一个response对象，让爬虫进行处理， 而这个response对象是这样的

```python
>>> type(response)
<class 'scrapy.http.response.html.HtmlResponse'>

# 这里body等同于requests库中的content 二进制字节流
response.text = response.body.decode(response.encoding)
```

因此我们可以找到该对象 **HtmlResponse <- TextResponse <- Response**

HtmlResponse对象继承自TextResponse，他俩完全是一样的，那我们就来看看TextResponse吧

**Scrapy/http/response/text.py** 中的 **TextResponse**类，其中属性encoding（@property可以把方法变成属性）就是我们想要的，可以看到，它是通过2个方法获得，`_declared_encoding()`和 `_body_inferred_encoding()`

`_declared_encoding()`其实就是从3个方面获取编码，`_init__`方法里的属性，其次是请求头里的编码，最后就是`_body_declared_encoding` 这个就是从html页面中提取的编码格式，如果继续看源码的话，就会发现其实就是meta中获取的。 这3个方式就是所谓的声明好的编码，接下来的就是从body从推断出来的编码

![scrapy编码1](http://rottengeek.github.io/img/scrapy/scrapy编码1.jpg)
这个函数其实就是从body进行推断，如用不同的编码进行尝试。

![scrapy编码2](http://rottengeek.github.io/img/scrapy/scrapy编码2.jpg)

## 总结

自此，关于编码解码的内容讲了一丢丢，纯当抛砖引玉，有兴趣的同学还可以深入看下源码。在一般情况下，我们爬取中文网站，其实无非就是utf-8，或者gb18030编码（gbk，gb2312的父集）。scrapy中也做过编码的映射，发现是gbk或者gb2312都会统一成gb18030。那么我们不妨模仿scrapy的源码，如下

```python
text = ''

for enc in ('utf-8', 'gb18030'):
  try:
    text = response.content.decode(enc)
  except UnicodeDecodeError
  	continue
if not text:
  text = response.text
```

这样其实已经可以处理大部分中文网站，但是缺点是效率比较低，每次都要解码。

